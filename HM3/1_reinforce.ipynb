{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in pytorch (5 pts)\n",
    "\n",
    "Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"bash\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a901bdd888>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits. \n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "model = nn.Sequential(\n",
    "  #<YOUR CODE: define a neural network that predicts policy logits>\n",
    "    nn.Linear(state_dim[0], 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, n_actions)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
    "So, here gradient calculation is not needed.\n",
    "<br>\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "<br>\n",
    "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
    "<br>\n",
    "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
    "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "<br>\n",
    "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    torch_states = torch.as_tensor(states, dtype=torch.float32)\n",
    "    probas = F.softmax(model(torch_states), dim=1)\n",
    "    \n",
    "    return probas.detach().numpy()\n",
    "    \n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    #<YOUR CODE>\n",
    "    #return <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "assert isinstance(\n",
    "    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (\n",
    "    test_states.shape[0], env.action_space.n), \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1),\n",
    "                   1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\" \n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions andrewards\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        #a = <YOUR CODE>\n",
    "        a = np.random.choice(n_actions, p=action_probs) \n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1] [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(actions, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    # <YOUR CODE>\n",
    "    # return <YOUR CODE: array of cumulative rewards>\n",
    "    \n",
    "    G = [rewards[-1]]\n",
    "    \n",
    "    for r in rewards[-2::-1]:\n",
    "        G.append(r + gamma * G[-1])\n",
    "    return G[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
    "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over T } \\sum_{i=1}^T  G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over T } \\sum_{i=1}^T \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "Entropy Regularizer\n",
    "  $$ H = - {1 \\over T} \\sum_{i=1}^T  \\sum_{a \\in A} {\\pi_\\theta(a|s_i) \\cdot \\log \\pi_\\theta(a|s_i)}$$\n",
    "\n",
    "$T$ is session length\n",
    "\n",
    "So we optimize a linear combination of $- \\hat J$, $-H$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "#optimizer = <YOUR CODE>\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(\n",
    "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
    "   \n",
    "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
    "    #entropy = <YOUR CODE>\n",
    "    #loss = <YOUR CODE>\n",
    "    J = torch.mean(log_probs_for_actions * cumulative_returns)\n",
    "    entropy = -(probs * log_probs).sum(-1).mean()\n",
    "    loss = - J - entropy_coef * entropy\n",
    "\n",
    "    # Gradient descent step\n",
    "    #<YOUR CODE>\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\.conda\\envs\\HM\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:24.510\n",
      "mean reward:32.620\n",
      "mean reward:33.210\n",
      "mean reward:37.260\n",
      "mean reward:93.680\n",
      "mean reward:174.980\n",
      "mean reward:87.360\n",
      "mean reward:331.640\n",
      "mean reward:116.140\n",
      "mean reward:121.380\n",
      "mean reward:81.860\n",
      "mean reward:436.660\n",
      "mean reward:826.950\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env))\n",
    "               for _ in range(100)]  # generate new sessions\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "monitor_env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\", force=True)\n",
    "sessions = [generate_session(monitor_env) for _ in range(100)]\n",
    "monitor_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.8028.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\" + video_names[-1]))  # this may or may not be the _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
